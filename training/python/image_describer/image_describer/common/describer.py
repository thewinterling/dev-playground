from pathlib import Path
from typing import List
from PIL import Image
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from urllib.parse import urlparse
import requests


def is_valid_url(url: str) -> bool:
    """Checks if the given string 'url' is a valid URL (format-wise).
    It does not check if the address really exists.

    Returns: True if the scheme matches the one of a regular url, False otherwise."""
    try:
        parsed = urlparse(url)
        return all([parsed.scheme, parsed.netloc])
    except ValueError:
        return False


class Describer:
    MODEL_NAME = "Salesforce/blip-image-captioning-base"

    def __init__(self, images: List[str] | List[Path]):
        """This is a simple try to use one of the many image-captioning models that
        are available on huggingface - pretrained. The model used here is BLIP.
        For more info on the model, see: https://huggingface.co/Salesforce/blip-image-captioning-base.

        Given that it is only a few lines of not too-complex code it is quite impressive what the model can do.
        However in absolute terms, rerformance is not optimal. Might still be a  good starting point
        for a simple image captioning tool.

        Args:
            images: List of either local `pathlib.Path`s or list of str to image URLs.
        """
        self._images = images

        # For the start, we use the BLIP model and processor for describing the images.
        # To be decided if that should also be made configurable.
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._model = BlipForConditionalGeneration.from_pretrained(Describer.MODEL_NAME)
        self._processor = BlipProcessor.from_pretrained(Describer.MODEL_NAME)

    def describe_images(self) -> str:
        """Runs the image description using a BLIP model. The function is capable of opening images from a local
        folder as well as from an URL.

        Args:
            None

        Returns:
            dict: Description of the image(s) as generated by the model as key value pair of "image name: caption".
        """
        captions = {}
        for image in self._images:
            if is_valid_url(image):
                opened_image = Image.open(requests.get(image, stream=True).raw).convert(
                    "RGB"
                )
            else:
                opened_image = Image.open(image).convert("RGB")
            inputs = self._processor(
                images=opened_image,
                return_tensors="pt",
            ).to(self._device)

            with torch.no_grad():
                caption = self._model.generate(**inputs)

            captions[image] = self._processor.decode(
                caption[0], skip_special_tokens=True
            )
        return captions
